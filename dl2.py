# -*- coding: utf-8 -*-
"""DL2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pfPFUepQjIU7jztj9i_gWa9PNrMSG5gk
"""

pip install pandas numpy tensorflow

!pip install datasets

# 1. Load Required Libraries
import pandas as pd
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense
from tensorflow.keras.optimizers import Adam

# 2. Load and Clean the Dataset
train_df = pd.read_csv('/content/hi.translit.sampled.train.tsv', sep='\t', header=None, encoding='utf-8')
test_df = pd.read_csv('/content/hi.translit.sampled.test.tsv', sep='\t', header=None, encoding='utf-8')
dev_df = pd.read_csv('/content/hi.translit.sampled.dev.tsv', sep='\t', header=None, encoding='utf-8')

# Drop unwanted third column and rename
train_df = train_df.drop(columns=[2])
test_df = test_df.drop(columns=[2])
dev_df = dev_df.drop(columns=[2])

train_df.columns = ['devanagari', 'latin']
test_df.columns = ['devanagari', 'latin']
dev_df.columns = ['devanagari', 'latin']

# Drop NaNs and convert to strings
for df in [train_df, test_df, dev_df]:
    df.dropna(inplace=True)
    df['latin'] = df['latin'].astype(str)
    df['devanagari'] = df['devanagari'].astype(str)

print("Train Sample:\n", train_df.head())

# 3. Tokenize the Texts
latin_tokenizer = Tokenizer(char_level=True)
latin_tokenizer.fit_on_texts(train_df['latin'])

devanagari_tokenizer = Tokenizer(char_level=True)
devanagari_tokenizer.fit_on_texts(train_df['devanagari'])

input_vocab_size = len(latin_tokenizer.word_index) + 1
output_vocab_size = len(devanagari_tokenizer.word_index) + 1

print(f"Latin vocab size: {input_vocab_size}")
print(f"Devanagari vocab size: {output_vocab_size}")

# 4. Prepare the Data for Training
train_input_seq = latin_tokenizer.texts_to_sequences(train_df['latin'])
train_output_seq = devanagari_tokenizer.texts_to_sequences(train_df['devanagari'])

max_input_length = max(len(seq) for seq in train_input_seq)
max_output_length = max(len(seq) for seq in train_output_seq)

train_input_seq = pad_sequences(train_input_seq, maxlen=max_input_length, padding='post')
train_output_seq = pad_sequences(train_output_seq, maxlen=max_output_length, padding='post')

train_output_seq_one_hot = np.array([
    to_categorical(seq, num_classes=output_vocab_size)
    for seq in train_output_seq
])

print(f"Train Input Shape: {train_input_seq.shape}")
print(f"Train Output Shape (one-hot): {train_output_seq_one_hot.shape}")

# 5. Define the Seq2Seq Model
def define_model(input_vocab_size, output_vocab_size, input_timesteps, output_timesteps, embedding_dim, hidden_units):
    encoder_inputs = Input(shape=(input_timesteps,))
    encoder_embedding = Embedding(input_vocab_size, embedding_dim)(encoder_inputs)
    encoder_outputs, state_h, state_c = LSTM(hidden_units, return_state=True)(encoder_embedding)
    encoder_states = [state_h, state_c]

    decoder_inputs = Input(shape=(output_timesteps,))
    decoder_embedding = Embedding(output_vocab_size, embedding_dim)(decoder_inputs)
    decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)
    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)
    decoder_dense = Dense(output_vocab_size, activation='softmax')
    decoder_outputs = decoder_dense(decoder_outputs)

    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

embedding_dim = 50
hidden_units = 256
model = define_model(input_vocab_size, output_vocab_size, max_input_length, max_output_length, embedding_dim, hidden_units)
model.summary()

# 6. Train the Model
decoder_input_train = np.zeros_like(train_output_seq)
decoder_input_train[:, 1:] = train_output_seq[:, :-1]

model.fit(
    [train_input_seq, decoder_input_train],
    train_output_seq_one_hot,
    batch_size=64,
    epochs=50,
    validation_split=0.2
)

# 7. Evaluate the Model
test_input_seq = latin_tokenizer.texts_to_sequences(test_df['latin'])
test_output_seq = devanagari_tokenizer.texts_to_sequences(test_df['devanagari'])

test_input_seq = pad_sequences(test_input_seq, maxlen=max_input_length, padding='post')
test_output_seq = pad_sequences(test_output_seq, maxlen=max_output_length, padding='post')

decoder_input_test = np.zeros_like(test_output_seq)
decoder_input_test[:, 1:] = test_output_seq[:, :-1]

test_output_seq_one_hot = np.array([
    to_categorical(seq, num_classes=output_vocab_size)
    for seq in test_output_seq
])

test_loss, test_acc = model.evaluate([test_input_seq, decoder_input_test], test_output_seq_one_hot)
print(f"Test Accuracy: {test_acc * 100:.2f}%")

# 8. Make Predictions
def decode_sequence(input_seq):
    decoder_input = np.zeros((1, max_output_length))
    for i in range(max_output_length):
        output_tokens = model.predict([input_seq, decoder_input], verbose=0)
        sampled_token_index = np.argmax(output_tokens[0, i, :])
        decoder_input[0, i] = sampled_token_index
    predicted_seq = decoder_input[0]
    predicted_text = ''.join([devanagari_tokenizer.index_word.get(int(i), '') for i in predicted_seq if i != 0])
    return predicted_text

# Predict on one test sample
sample_input = test_input_seq[0:1]
predicted_output = decode_sequence(sample_input)
print(f"\nLatin Input       : {test_df.iloc[0]['latin']}")
print(f"Actual Devanagari : {test_df.iloc[0]['devanagari']}")
print(f"Predicted Output  : {predicted_output}")

import pandas as pd
import re
from datasets import Dataset
from transformers import (
    GPT2Tokenizer, GPT2LMHeadModel,
    Trainer, TrainingArguments,
    DataCollatorForLanguageModeling,
    pipeline, set_seed
)

# ---------- Step 1: Load and Clean Lyrics ----------
def clean_lyrics(lyric):
    if pd.isna(lyric):
        return ""
    lyric = re.sub(r'^#+', '', str(lyric))
    lyric = lyric.encode('utf-8').decode('utf-8', 'ignore')
    lyric = re.sub(r'[\u2018\u2019\u201c\u201d]+', "'", lyric)
    lyric = re.sub(r'[^\x00-\x7F]+', '', lyric)
    return lyric.strip()

def load_and_clean_lyrics(file_paths):
    df = pd.concat([pd.read_csv(path) for path in file_paths], ignore_index=True)
    if 'Lyric' not in df.columns:
        raise ValueError("Missing 'Lyric' column.")
    df['Lyric'] = df['Lyric'].astype(str).apply(clean_lyrics)
    return df['Lyric'].dropna().tolist()

# ---------- Step 2: Tokenizer and Dataset ----------
def get_tokenizer_and_model(model_name="gpt2"):
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token
    model = GPT2LMHeadModel.from_pretrained(model_name)
    model.config.pad_token_id = tokenizer.pad_token_id
    return tokenizer, model

def prepare_dataset(lyrics, tokenizer, max_length=512):
    dataset = Dataset.from_dict({"text": lyrics})

    def tokenize(example):
        return tokenizer(
            example["text"],
            truncation=True,
            padding="max_length",
            max_length=max_length
        )

    dataset = dataset.map(tokenize, batched=True)
    dataset = dataset.map(lambda x: {"labels": x["input_ids"]}, batched=True)
    return dataset

# ---------- Step 3: Train ----------
def train_model(dataset, model, tokenizer):
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

    training_args = TrainingArguments(
        output_dir="./gpt2-lyrics-output",
        per_device_train_batch_size=2,
        num_train_epochs=3,
        logging_steps=50,
        save_steps=500,
        save_total_limit=1,
        report_to="none",
        prediction_loss_only=True,
        fp16=False  # Set True if using GPU w/ float16 support
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        tokenizer=tokenizer,
        data_collator=data_collator
    )

    trainer.train()

# ---------- Step 4: Generate ----------
def generate_lyrics(prompt="Under the neon lights, we danced"):
    tokenizer, model = get_tokenizer_and_model()
    generator = pipeline("text-generation", model=model, tokenizer=tokenizer)
    set_seed(42)
    output = generator(prompt, max_length=100, num_return_sequences=1)
    print("\nðŸŽ¶ Lyrics\n")
    print(output[0]["generated_text"])

# ---------- Main ----------
if __name__ == "__main__":
    file_paths = ["/content/ArianaGrande.csv", "/content/BillieEilish.csv"]
    lyrics = load_and_clean_lyrics(file_paths)
    tokenizer, model = get_tokenizer_and_model()
    dataset = prepare_dataset(lyrics, tokenizer)
    train_model(dataset, model, tokenizer)
    generate_lyrics(prompt="Under the neon lights, we danced")